{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install langchain langchain-openai langchain-core -q"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJP-9Yy4Xjeg",
        "outputId": "b52450ba-777b-41fe-b629-65ff4017422f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/76.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WARNING: Package(s) not found: langchain-community\n",
        "Name: langchain-core\n",
        "Version: 0.3.79\n",
        "Summary: Building applications with LLMs through composability\n",
        "Home-page:\n",
        "Author:\n",
        "Author-email:\n",
        "License: MIT\n",
        "Location: /usr/local/lib/python3.12/dist-packages\n",
        "Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, tenacity, typing-extensions\n",
        "Required-by: langchain, langchain-openai, langchain-text-splitters\n",
        "---\n",
        "Name: langchain\n",
        "Version: 0.3.27\n",
        "Summary: Building applications with LLMs through composability\n",
        "Home-page:\n",
        "Author:\n",
        "Author-email:\n",
        "License: MIT\n",
        "Location: /usr/local/lib/python3.12/dist-packages\n",
        "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
        "Required-by:"
      ],
      "metadata": {
        "id": "lwJJCpneYGqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain-community langchain-core langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMPkqiS8X7Et",
        "outputId": "4c94341b-5511-4c6c-d416-dc3f0f497b46"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: langchain-community\u001b[0m\u001b[33m\n",
            "\u001b[0mName: langchain-core\n",
            "Version: 0.3.79\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, tenacity, typing-extensions\n",
            "Required-by: langchain, langchain-openai, langchain-text-splitters\n",
            "---\n",
            "Name: langchain\n",
            "Version: 0.3.27\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksqqCWXiBg_X",
        "outputId": "337c367b-e5dc-47aa-9284-cf34d125d5d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Read the key from file\n",
        "with open(\"openai_key.txt\", \"r\") as f:\n",
        "    api_key = f.read().strip()\n",
        "\n",
        "# Set it as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "jzVHxTIABvbl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.tools import tool # Modern decorator for tools"
      ],
      "metadata": {
        "id": "ClL_FV0oY4kn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating the base llm object\n",
        "\n",
        "#from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "myllmObject = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")"
      ],
      "metadata": {
        "id": "uE2kdDf7aG5B"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating country-capital tool"
      ],
      "metadata": {
        "id": "7TTTtLDFdIzD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"What is the capital of {country}?\")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "countrycapitalchain = prompt | myllmObject | parser\n",
        "\n",
        "response = countrycapitalchain.invoke({\"country\": \"Japan\"})\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPUBOy9MZkqw",
        "outputId": "7760870d-2cfa-4161-a0b0-a38c3f70d91e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Japan is Tokyo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "# Assuming countrycapitalchain.run is your function to get capital of a country\n",
        "@tool\n",
        "def countrycapital_tool(query: str) -> str:\n",
        "    \"\"\"Useful for when you need to answer questions about a country and its capital.\"\"\"\n",
        "    #return countrycapitalchain.run(query)\n",
        "    return countrycapitalchain.invoke({\"country\":query})"
      ],
      "metadata": {
        "id": "SKbv-6PHc14n"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When giving tools to an LLM, pass as a list\n",
        "tools = [countrycapital_tool]"
      ],
      "metadata": {
        "id": "3K62Kz5_c7wr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools[0].name, tools[0].description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEJxT7R3bjtQ",
        "outputId": "008d8873-f631-4997-ff9a-c66b4071e754"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('countrycapital_tool',\n",
              " 'Useful for when you need to answer questions about a country and its capital.')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a small Math tool -- start"
      ],
      "metadata": {
        "id": "8pQhBfmRhAFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"Solve this equation {equation}?\")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "mathchain = prompt | myllmObject | parser\n",
        "\n",
        "response = mathchain.invoke({\"equation\": \"3x=9\"})\n",
        "print(response)"
      ],
      "metadata": {
        "id": "K5_L51SlxVdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed8260d-1683-4dc9-c7ca-30e5f4e9d87c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve the equation 3x = 9, we need to isolate the variable x.\n",
            "\n",
            "Divide both sides of the equation by 3:\n",
            "\n",
            "3x/3 = 9/3\n",
            "x = 3\n",
            "\n",
            "Therefore, the solution to the equation 3x = 9 is x = 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "# Assuming countrycapitalchain.run is your function to get capital of a country\n",
        "@tool\n",
        "def maths_tool(query: str) -> str:\n",
        "    \"\"\"Useful for when you need to answer qsimple equations.\"\"\"\n",
        "    return mathchain.invoke({\"equation\": query})\n"
      ],
      "metadata": {
        "id": "x9eip4VycfGy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When giving tools to an LLM, pass as a list\n",
        "tools.append(maths_tool)"
      ],
      "metadata": {
        "id": "1QW5eNTj0e-F"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# End of Creating Math tool -- End"
      ],
      "metadata": {
        "id": "Pi7yVV79jwEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tools[0].name, tools[0].description)\n",
        "print(tools[1].name, tools[1].description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCvXV6HXjRV0",
        "outputId": "22cd2958-b1ec-4590-d26a-94ae829aa284"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "countrycapital_tool Useful for when you need to answer questions about a country and its capital.\n",
            "maths_tool Useful for when you need to answer qsimple equations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#del tools[0]"
      ],
      "metadata": {
        "id": "dKgCkm9bkpWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1l8Xrsclxy8",
        "outputId": "38aba262-50f4-40a9-cb19-e43694a2cf5e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StructuredTool(name='countrycapital_tool', description='Useful for when you need to answer questions about a country and its capital.', args_schema=<class 'langchain_core.utils.pydantic.countrycapital_tool'>, func=<function countrycapital_tool at 0x7995ff2c98a0>),\n",
              " StructuredTool(name='maths_tool', description='Useful for when you need to answer qsimple equations.', args_schema=<class 'langchain_core.utils.pydantic.maths_tool'>, func=<function maths_tool at 0x7995ff2ca340>)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Initialize the LLM ---\n",
        "\n",
        "# Use a model that supports Tool Calling (e.g., gpt-4o-mini, gpt-4)\n",
        "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "myllmObject = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")"
      ],
      "metadata": {
        "id": "bIc4Orr1Yolx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This structure is critical for passing history and scratchpad\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an extremely helpful assistant. Use the 'maths_tool' tool only when asked about the linear equations and  countrycapital_tool  tool for country capital\"),\n",
        "        # This is where the Agent Executor injects past messages (memory)\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        # This is the new user input\n",
        "        (\"human\", \"{input}\"),\n",
        "        # This is where the Agent Executor injects the LLM's thoughts and tool observations\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Lw-NMZMWYxat"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Assemble the Agent (LCEL / Pipeline) ---\n",
        "\n",
        "# A. Create the Agent: The runnable that decides what action to take (LLM + Tools + Prompt)\n",
        "agent = create_tool_calling_agent(myllmObject, tools, prompt)\n",
        "\n",
        "# B. Create the Executor: The runtime that manages the loop (Agent + Execution logic)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=False # See the reliable tool-calling JSON flow\n",
        ")"
      ],
      "metadata": {
        "id": "OMxUtmcTZIgw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Define Conversation History (Memory) ---\n",
        "chat_history = [\n",
        "    HumanMessage(content=\"Hello! My name is John.\"),\n",
        "    AIMessage(content=\"Hello John! How can I help you today?\"),\n",
        "]"
      ],
      "metadata": {
        "id": "1SA2my4SZOx3"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Run the Agent ---\n",
        "\n",
        "print(\"\\n--- Turn 1: Tool Use ---\")\n",
        "result_1 = agent_executor.invoke({\n",
        "    \"input\": \"Solve the linear 3x+9=6\",\n",
        "    \"chat_history\": chat_history # Pass the history explicitly\n",
        "})\n",
        "print(f\"\\nðŸ¤– AI Response 1: {result_1['output']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVK1mIoZZR93",
        "outputId": "659289f5-31b2-43bd-ee2a-0d3c9dbf071d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Turn 1: Tool Use ---\n",
            "\n",
            "ðŸ¤– AI Response 1: The solution to the linear equation 3x + 9 = 6 is x = -1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.extend([\n",
        "    HumanMessage(content=\"What is the capital of united Kingdom?\"),\n",
        "    AIMessage(content=result_1['output'])\n",
        "])"
      ],
      "metadata": {
        "id": "_UPFZuMHbWvr"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Turn 2: Tool Use ---\")\n",
        "result_2 = agent_executor.invoke({\n",
        "    \"input\": \"What is the capital of United Kingdom\",\n",
        "    \"chat_history\": chat_history # Pass the updated history\n",
        "})\n",
        "print(f\"\\nðŸ¤– AI Response 2: {result_2['output']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3gX6OtabxK-",
        "outputId": "e8eddfef-3efc-4b48-8951-8b623bf7dc74"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Turn 2: Tool Use ---\n",
            "\n",
            "ðŸ¤– AI Response 2: The capital of the United Kingdom is London.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Turn 3: Memory Check (No Tool Use) ---\")\n",
        "result_3= agent_executor.invoke({\n",
        "    \"input\": \"What was the name I told you?\",\n",
        "    \"chat_history\": chat_history # Pass the updated history\n",
        "})\n",
        "print(f\"\\nðŸ¤– AI Response 2: {result_3['output']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGtcGB9Ccn1-",
        "outputId": "b771a8bc-3bac-40b3-dfcf-4b2370c87da3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Turn 3: Memory Check (No Tool Use) ---\n",
            "\n",
            "ðŸ¤– AI Response 2: You told me your name is John.\n"
          ]
        }
      ]
    }
  ]
}
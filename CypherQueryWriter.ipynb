{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9cayp5kucyM",
        "outputId": "f38330e1-8ff2-46bb-bfe6-1e7cbb8b7793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/204.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.9/328.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.17.0 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.1.2 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain  langchain_community langchain-openai langchain_neo4j  -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "1ABYLqwHuqPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_properties(path):\n",
        "    data = {}\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            if \"=\" in line:\n",
        "                k, v = line.split(\"=\", 1)\n",
        "                data[k.strip()] = v.strip()\n",
        "    return data"
      ],
      "metadata": {
        "id": "PPNFpT-lutdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "props = load_properties(\"openai_key.txt\")\n",
        "\n",
        "os.environ[\"OPENAI_KEY\"] = props[\"OPENAI_KEY\"]\n",
        "os.environ[\"NEO4J_URI\"] = props[\"NEO4J_URI\"]\n",
        "os.environ[\"NEO4J_USERNAME\"] = props[\"NEO4J_USERNAME\"]\n",
        "os.environ[\"NEO4J_PASSWORD\"] = props[\"NEO4J_PASSWORD\"]\n",
        "#os.environ[\"NEO4J_DATABASE\"] = props[\"NEO4J_DATABASE\"]"
      ],
      "metadata": {
        "id": "d7n2czhDuvk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    openai_api_key=os.getenv(\"OPENAI_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "gfFrBY61u5FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = Neo4jGraph(\n",
        "    url=os.getenv(\"NEO4J_URI\"),\n",
        "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
        "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "shAP_7Qou9oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Type -1\n",
        "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
        "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
        "Convert the user's question based on the schema.\n",
        "\n",
        "Schema: {schema}\n",
        "Question: {query}\n",
        "\"\"\"\n",
        "\n",
        "cypher_generation_prompt = PromptTemplate(\n",
        "    template=CYPHER_GENERATION_TEMPLATE,\n",
        "    input_variables=[\"schema\", \"query\"],\n",
        ")"
      ],
      "metadata": {
        "id": "NXrPH0TIvBsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Type -2 : provide specific instructions to the LLM to state that the generated Cypher statements should follow the schema.\n",
        "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
        "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
        "Convert the user's question based on the schema.\n",
        "\n",
        "Instructions:\n",
        "Use only the provided relationship types and properties in the schema.\n",
        "Do not use any other relationship types or properties that are not provided.\n",
        "Only respond to questions that require you to construct a Cypher statement.\n",
        "If no data is returned, do not attempt to answer the question.\n",
        "\n",
        "Schema: {schema}\n",
        "Question: {question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Lj7PTyweGqL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type -3 : Few-shot examples\n",
        "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
        "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
        "Convert the user's question based on the schema.\n",
        "\n",
        "Instructions:\n",
        "Use only the provided relationship types and properties in the schema.\n",
        "Do not use any other relationship types or properties that are not provided.\n",
        "For movie titles that begin with \"The\", move \"the\" to the end, For example \"The 39 Steps\" becomes \"39 Steps, The\" or \"The Matrix\" becomes \"Matrix, The\".\n",
        "\n",
        "If no data is returned, do not attempt to answer the question.\n",
        "Only respond to questions that require you to construct a Cypher statement.\n",
        "Do not include any explanations or apologies in your responses.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Find movies and genres:\n",
        "MATCH (m:Movie)-[:IN_GENRE]->(g)\n",
        "RETURN m.title, g.name\n",
        "\n",
        "Schema: {schema}\n",
        "Question: {question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p_amE9qJITzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cypher_chain = GraphCypherQAChain.from_llm(\n",
        "    llm,\n",
        "    graph=graph,\n",
        "    cypher_prompt=cypher_generation_prompt,\n",
        "    verbose=True,\n",
        "    allow_dangerous_requests=True\n",
        ")"
      ],
      "metadata": {
        "id": "gxNb2fqmvHPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = cypher_chain.invoke({\"query\": \"What is the plot of the movie DDLG?\"})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m10nhVVrvKVf",
        "outputId": "a3c92e9a-c049-4c94-c13c-a60f90c3b7a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (m:Movie {title: \"DDLG\"})\n",
            "RETURN m.plot\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'query': 'What is the plot of the movie DDLG?', 'result': \"I don't know the answer.\"}\n"
          ]
        }
      ]
    }
  ]
}
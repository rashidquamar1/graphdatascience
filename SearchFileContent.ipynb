{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Upload documents to Neo4j, make into chunk. Create embeddings of those chunks\n",
        "Search content from within files using Vector Index made out of chunk embedding"
      ],
      "metadata": {
        "id": "xqi-svcq8I9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai langchain_neo4j  -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nr8DZcy8NF-",
        "outputId": "611d3cfd-94c3-4b2e-d130-b7a94b5eb36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/204.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.9/328.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.17.0 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.1.2 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_properties(path):\n",
        "    data = {}\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            if \"=\" in line:\n",
        "                k, v = line.split(\"=\", 1)\n",
        "                data[k.strip()] = v.strip()\n",
        "    return data"
      ],
      "metadata": {
        "id": "XkE521iB8Xl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "props = load_properties(\"openai_key.txt\")\n",
        "\n",
        "os.environ[\"NEO4J_URI\"] = props[\"NEO4J_URI\"]\n",
        "os.environ[\"OPENAI_KEY\"] = props[\"OPENAI_KEY\"]\n",
        "os.environ[\"NEO4J_USERNAME\"] = props[\"NEO4J_USERNAME\"]\n",
        "os.environ[\"NEO4J_PASSWORD\"] = props[\"NEO4J_PASSWORD\"]\n",
        "#os.environ[\"NEO4J_DATABASE\"] = props[\"NEO4J_DATABASE\"]"
      ],
      "metadata": {
        "id": "gJAbB8QC8aeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
        "from langchain.schema import Document\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA6vULzJ8iQQ",
        "outputId": "d09cf275-d258-41cc-dfe3-2a7fcf138e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"TextFiles/\"\n",
        "\n",
        "documents = []\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "    if os.path.isfile(file_path) and filename.endswith(\".txt\"):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "        doc = Document(\n",
        "            page_content=content,\n",
        "            metadata={\n",
        "                \"source\": \"local\",\n",
        "                \"filename\": filename,\n",
        "                \"path\": file_path\n",
        "            }\n",
        "        )\n",
        "\n",
        "        documents.append(doc)\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents\")"
      ],
      "metadata": {
        "id": "ZPJaviNlAPzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a6b8ce8-9bfa-4dda-c843-db69e14f4aa4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Service used to create the embeddings\n",
        "embedding_provider = OpenAIEmbeddings(\n",
        "    openai_api_key=os.getenv(\"OPENAI_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "gaeYymk4Crch"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = Neo4jGraph(\n",
        "    url=os.getenv(\"NEO4J_URI\"),\n",
        "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
        "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
        ")"
      ],
      "metadata": {
        "id": "0U6nDf69CxRh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_vector_Index = Neo4jVector.from_documents(\n",
        "    documents,\n",
        "    embedding_provider,\n",
        "    graph=graph,\n",
        "    index_name=\"myVectorIndex\",\n",
        "    node_label=\"Chunk\",\n",
        "    text_node_property=\"text\",\n",
        "    embedding_node_property=\"embedding\",\n",
        "    create_id_index=True,\n",
        ")"
      ],
      "metadata": {
        "id": "y2EzxDGAC1Iq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Retrieval Chain"
      ],
      "metadata": {
        "id": "LKKP1thhDvXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "qQkS4k10FP7U"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myllmObject = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    api_key=os.getenv(\"OPENAI_KEY\")\n",
        "  )"
      ],
      "metadata": {
        "id": "y-r7fCCUFWSj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filecontent_retriever = RetrievalQA.from_llm(\n",
        "    llm=myllmObject,\n",
        "    retriever=new_vector_Index.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "dya4Z_a5F_JY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = filecontent_retriever.invoke(\n",
        "    {\"query\": \"find the name of file having text as file 18\"}\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnguFBpTGLFo",
        "outputId": "73a37671-387c-4ee4-f851-70eb6a30a6fc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'find the name of file having text as file 18', 'result': 'I\\'m sorry, but based on the provided context, there is no mention of a file with the text \"file 18.\"'}\n"
          ]
        }
      ]
    }
  ]
}